---
title: "Ретро реинжиниринга Проекта Э - организация"
date: 2023-06-13T01:25:37+07:00
draft: false
---
:source-highlighter: rouge
:rouge-theme: github
:icons: font
:sectlinks:
:imagesdir: /drafts/project-e-retro/images

== Введение

Представьте ситуацию.
На дворе 2022 год, вы - техлид/архитектор с 17 годами опыта разработки бакэндов под JVM.
Вас просят взять и довести до прода проект на 9 человеко/месяцев оригинальной разработки, на 30 таблиц и ~100 эндпоинтов REST API.
Проект - готов на 90%, но есть нюансы:

. Проект на .net, на стеке 2019 года;
. В проекте нет ни одного теста;
. Связи с изначальными авторами проекта нет от слова совсем, технической документации к проекту - тоже.

Ваши действия?

Я, оказавшись в этой ситуации, решил переписать всё к едрене фене.
Силами команды из двух юниоров и одного стажра.
Официально я это назвал полным реинжинирингом проекта.

В этом посте я расскажу почему я решил всё переписать, как продавал эту идею и организовывал процесс и что из этого всего получилось.
Забегая вперёд, скажу что это история с хэппи эндом - я уложился в исходную оценку и вдвое сократил средние трудозатраты и количество багов на фичу.

== Проект Э

В начале января 2019 года один из аутсорсеров на бескрайнем пространстве СНГ начал работы над проектом по заказу российского разработчика медоборудования.
Далее я буду называть этот проект Проектом Э.

Очень упрощённо проект можно считать медицинским дневником.
Пользователи (пациенты) собирают с помощью устройства заказчика определённые показатели организма, которые сохраняются в БД.
Кроме того, пользователи вносят в систему дополнительную информацию, такую как приём лекарств, пищи, занятия спортом и т.п.
А врачи, по приглашению пациентов, получают доступ к их дневникам.

Так выглядела диаграмма контекста https://c4model.com/[модели C4] Проекта Э в конце 2019 года (картинка кликабельна):


image::project-e-context.drawio.svg[Диаграмма контекста Проекта Э,link={imagesdir}/project-e-context.drawio.svg]

Изначальная команда реализовала 90% проекта к октябрю 2019 года и заказчик ушёл на сертификацию своего устройства, заморозив разработку.

Но потом в марте 2020 года что-то пошло не так (возможно вы уже забыли, что у нас была такая штука как пандемия) и сертификация затянулась до 2022 года.

А в феврале 2022 года что-то пошло совсем не так (уверен, что тут вы ещё ничего не забыли) и изначальная команда перекинула репозитори проекта через забор с запиской, что они отказываются продолжать работы.

После этого заказчик начал искать нового подрядчика и в апреле 2022 года выбрал компанию Сибериан.Про.

В апреле-мае были преимущественно организационные работы и работы по подготовке мобильных приложений к публикации в сторы, а я подключился к проекту в июне в качестве техлида проекта.
И сразу понял, что вляпался я неподецки.

== Проблемы проекта


На второй день ознакомления с кодовой базой я в Slack-канал проекта написал такое сообщение:

> Я нашёл. оно никогда не перестанет быть смешным и не потеряет актуальность
https://thecodinglove.com/content/039/gez23qJ.webm

Вот сразу мемасик из ссылки:

++++
<video controls autoplay>
    <source src="https://thecodinglove.com/content/039/gez23qJ.webm
" type="video/webm">
    Your browser does not support the video tag.
</video>
++++

В тот же день я завёл в слаке канал project-e-wtf - куда сливал свой яд от всевозможных находок в коде.

Находок было очень много, но наибольший WTF у меня вызывали три штуки:

. Полное отсутствие каких бы то ни было тестов;
. Неуместное применение микросервисной архитектуры и её плохая реализация;
. Использование вертикальной архитектуры на базе MediatR.

Разберу их чуть подробнее.

=== Отсутствие тестов

В кодовой базе не было ни одного теста.
И если отсутствие функциональных и интеграционных тестов можно объяснить сложностью тестирования микросервисной архитектуры, то отсутствие даже юнит тестов на моках - для меня загадка, проектов без тестов я не видел с 2014 года.

В итоге, как только мы начали пытаться вносить изменения в систему - на нас тут же широкой рекой хлынули баги.

===  Микросервисы

Изначально диаграмма контейнеров бэка проекта в нотации C4 выглядела так:

image::project-e-retro-backend.drawio.svg[Диаграмма контейнеров бакэнда Проекта Э,link={imagesdir}/project-e-retro-backend.drawio.svg]

При том в проекте не было ни одной объективной причины, которая бы оправдывала сложности в разработке, привносимые микросервисной архитектурой.

Изначальная бек-команда состояла из одного бэк-разработчика, соотвественно банально не было людей, между которыми надо было минимизировать коммуникации и конфликты.

Различных требований к масштабированию частей системы тоже не было и не предвидится.
На тот момент, когда я принял проект, он ещё не был зарелизан, и соответственно нагрузка на него была 0 rps, и все сервисы деплоились в единственном экземпляре.
Сейчас, спустя 8 месяцев опытной эксплуатации, у нас пиковая нагрузка составляет 0.5 rps.
А после выхода на расчётное количество пользователей, планируемая средняя нагрузка будет порядка 10-20 rps, а пиковая не превысит 100 rps.
Соотвественно, никаких проблем с масштабированием не предвиделось и не предвидится - пара инстансов, созданных из соображений высокой доступности с лёгкостью будет закрывать весь входящий поток запросов.

Единственное место, где ожидается сильный перекос - это база данных дневников.
Там мы ожидаем сотни миллионов-миллиарды строк, против десятков-сотен тысяч строк в остальных БД.
Но это будет в далёком и прекрасном будущем, а пока что их 300К с ростом на 3К в день.

Инфраструктура тоже были типовая и общая для большинства сервисов - PostgreSQL, RabbitMQ и всё.
Только у модуля email-нотификаций была уникальная зависимость на smtp-транспорт.

Никаких других факторов, требующих такого уровня изоляции частей системы, тоже не было.

Но неуместное использование микросервисов - было только половиной беды.
Исполнение также хромало на обе ноги.

Во-первых, в проекте использовался разделяемый модуль shared - антипаттерн разработки МСов.
Который среди прочего включал ДТОшки АПИ сервисов.
Соответственно разработка фичи которая затрагивала несколько сервисов (а таких фич было большинство) зачастую состояла из следующих шагов:

. Обновить модуль shared;
. Собрать и опубликовать его;
. Попытаться обновить сервер, обнаружить проблему в интерфейсе и вернуться на шаг 1;
. Попытаться обновить клиент, обнаружить проблему в интерфейсе и вернуться на шаг 1;
. Задеплоить сервер;
. Задеплоить клиент.

Отдельную пикантность ситуации придавало наличие сервиса share, который отвечал за представление доступа к данным пациентов - я не сразу заучил кто из них кто.

Во-вторых, микросервисы, опять же вопреки основополагающему принципу их дизайна, обладали высокой степенью сцепленности - практически каждая операция включала в себя синхронные обращения к другим микросервисам, которые, в процессе обработки запросов снова шли в следующие микросервисы.

Например, вот так выглядит граф вызовов в юзкейсе предпросмотра группы пациентов:

image::project-e-retro-create-group.drawio.svg[Диаграмма контекста Проекта Э,link={imagesdir}/project-e-retro-create-group.drawio.svg]

Юзкейс заключается в том, что администраторы могут создавать группы из пациентов, наблюдаемых определёнными врачами.
При создании новой группы на первом этапе (синие стрелки) выполняется выбор врачей с поиском по емейлу, а потом отображается состав группы для предпросмотра (зелёные стрелки).

А так выглядела генерация PDF-отчёта по наблюдаемому:

image::project-e-retro-generate-pdf-report.drawio.svg[Диаграмма контекста Проекта Э,link={imagesdir}/project-e-retro-generate-pdf-report.drawio.svg]

Знаю, что некоторые эксперты по МСА считают такие графы сетевых вызовов нормой, но на мой взгляд это совершенно не эргономично и соотвественно не должно быть нормой.

В результате, у команды были все сложности, свойственные микросервисной архитектуре, но она не нуждалась ни в одном из преимуществ, которые даёт МСА.

===  Вертикальная архитектура на базе MediatR

Это спорная тема и знаю, что такой подход популярен в .net-сообществе, однако мне он не нравится.
Для вертикальной архитектуры не существует единого определения и можно нагуглить множество разных вариаций её реализации.
Вариант, который был использован в Проекте Э, довольно подробно описан в https://medium.com/@yurexus/mediatr-outside-vertical-slice-architecture-and-why-you-are-probably-using-it-wrong-3bfd45b0fe0e[этом посте].

Если вкратце, то использованный подход можно охарактеризовать так:

. На каждую операцию в слое сервисов заводится отдельный класс-обработчик;
. Доступ к данным размазан между репозиториями (модификация через EntityFramework) и обработчиками (чтение через строковые константы с SQL);
. Контроллеры вместо прямого вызова сервисов отправляют команду в MediatR и он сам как-то определяет в какой класс-обработчик её передать.

В итоге типичный обработчик выглядел так:

[source,csharp]
----
namespace ProjectE.Share.Api.Controllers.Queries.GetObservables
{
    public class GetObservablesQueryHandler : IRequestHandler<GetObservablesQuery, GetObservablesQueryResult>
    {

        // Поля и конструктор

        public async Task<GetObservablesQueryResult> Handle(GetObservablesQuery request, CancellationToken cancellationToken)
        {
            var startIndex = request.PageSize * (request.PageIndex - 1);
            const string sql = @"select count(*)
                                  from observers o
                                 where o.user_id = @userId and not o.is_deleted;
                                 select o.observable_id, obs.user_id
                                  from observers o
                                 inner join observables obs on obs.id = o.observable_id
                                 where o.user_id = @userId and not o.is_deleted
                                 limit @pageSize offset @startIndex";

            var result = new ObservablesQueryResultDto {Meta = new MetaDataDto {CurrentPage = request.PageIndex, PageSize = request.PageSize}};
            using (var connection = new NpgsqlConnection(_options.Value.ConnectionString))
            {
                await connection.OpenAsync(cancellationToken);
                using (var multi = await connection.QueryMultipleAsync(sql,
                           new
                           {
                               userId = request.UserId,
                               pageSize = request.PageSize,
                               startIndex
                           }))
                {
                    result.Meta.TotalItems = await multi.ReadFirstAsync<long>();
                    result.Items = await ParseObservables(await multi.ReadAsync<dynamic>());
                }
            }

            return new GetObservablesQueryResult(result);
        }

        // Вспомогательные методы маппинга данных

    }
}
----

А а в соседней директории был какой-нибудь такой код:

[source,csharp]
----

// Аналогичный "заголовок"

public async Task<GetObservablesBySearchQueryResult> Handle(GetObservablesBySearchQuery request,
    CancellationToken cancellationToken)
{
    var startIndex = request.PageSize * (request.PageIndex - 1);
    const string sql = @"select o.observable_id, obs.user_id
                         from observers o
                            inner join observables obs on obs.id = o.observable_id
                         where o.user_id = @userId and not is_deleted
                         limit @pageSize offset @startIndex";

    var result = new ObservablesQueryResultDto { Meta = new MetaDataDto { CurrentPage = request.PageIndex, PageSize = request.PageSize } };

    using (var connection = new NpgsqlConnection(_options.Value.ConnectionString))
    {
        await connection.OpenAsync(cancellationToken);
        using (var multi = await connection.QueryMultipleAsync(sql,
                   new
                   {
                       userId = request.UserId,
                       pageSize = 100,
                       startIndex
                   }))
        {
            result.Items = await ParseObservables(await multi.ReadAsync<dynamic>(), request.Search);
            result.Meta.TotalItems = result.Items.Length;
        }
    }

    return new GetObservablesBySearchQueryResult(result);
}

// Аналогичный "футер"

----

А в "двоюродной" директории был такой код:

[source,csharp]
----

namespace ProjectE.Share.Api.Controllers.Commands.UpdateObserverCustomData
{
    public class UpdateObserverCustomDataCommandHandler : IRequestHandler<UpdateObserverCustomDataCommand, UpdateObserverCustomDataCommandResult>
    {

        // Аналогичный "заголовок"

        public async Task<UpdateObserverCustomDataCommandResult> Handle(UpdateObserverCustomDataCommand command, CancellationToken cancellationToken)
        {
            var observable = await _unitOfWork.ObservableRepository.GetObservableByUserId(command.UserId);
            if (observable == null) return new UpdateObserverCustomDataCommandResult(CustomStatusCodes.NotFoundUserAccount, new[] { "Not found user observable account." });
            var result = await ChangeObserverCustomName(observable, command.CustomName, command.InviteId, cancellationToken);

            if (!result)
                _logger.LogError($"Can't change observer #{command.InviteId} custom name");

            return new UpdateObserverCustomDataCommandResult(result);
        }

        // Аналогичный "футер"
    }
}

namespace ProjectE.Share.Db.Repositories
{
    public class ObservableRepository : IObservableRepository
    {

        public async Task<Observable> GetObservableByUserId(int userId)
        {
            return await _context.Set<Observable>()
                .Include(o => o.Invites)
                    .ThenInclude(o=>o.Status)
                .Include(o => o.Observers)
                .SingleOrDefaultAsync(o => o.UserId == userId);
        }

    }
}

----

Тут надо обратить внимание на то, что доступ к данным в двух классах содержался в строковых константах с SQL-ем, а в одном - в LINQ-выражении.

Соотвественно, из-за этой размазанности логики доступа к данным вкупе с отсутствием тестов баги из серии "забыли поправить SQL в одном из слайсов" были у нас практически в каждом изменении.

MediatR же на этом фоне был мелким раздражителем, который приводил к:

. Усложнению навигации по коду - вместо прыжка через метод, приходилось выполнять поиск по команде;
. Необходимости на каждую операцию заводить по этой команде и её результату, даже если на вход подётся один int, а на выход идёт один boolean;

После двух месяцев страданий, у меня родилась гениальная идея:

image::the-idea.png[]

.Что я вынес для себя
[sidebar]
****
. https://www.martinfowler.com/bliki/MonolithFirst.html[Фаулер], https://www.oreilly.com/library/view/building-microservices/9781491950340/[Ньюман] и https://microservices.io/post/microservices/patterns/2020/10/18/microservices-are-a-mistake.html[Ричардсон] прав и проекты надо начинать с монолита;
. Брать на поддержку проекты без тестов только при условии, что каждая задача на разработку начинается с покрытия тестами релевантного кода сколько бы это не стоило.
. Мне вертикальная архитектура не подходит, можно на неё больше не смотреть.
****

== История продажи реинжиниринга

На самом деле идея перевести проект на Kotlin у меня появилась с самого начала, так как я сам специализируюсь на JVM и у меня была крутая команда, а найти адекватного специалиста по .net не получалось.
Но первое время дело дальше разговоров не шло.

Однако за два месяца активных работ (точнее попыток активной работы) не только я понял, что так продолжаться дальше не может, но и РП.
И 5 августа в треде о том, что уже второй дотнетчик делает задачи слишком долго, она написала:

> А как crazy idea - Леш, а переписать все на джава это сколько долго?

Я ушёл, посчитал количество таблиц и эндпоинтов, просуммировал их, получил ~120, добавил +/- 50% и ответил: 60 - 180 человеко/дней.

Затем, 11 августа я написал РП такое сообщение:

> Чёт не спится:)
Мне идея переписать на [line-through]#Джаве# Котлине кажется всё более разумной и реальной.
Из оценки в 100 дней - 50% это покрытие автоматическими тестами, что надо делать в любом случае, чтобы не помереть под регрессиями.
<...>
ну и у нас ещё есть переезд на свежий дотнет, который XXX оценил в 8 дней, и без тестов это скорее всего оптимистичная оценка.
Короч давай продавать эту авантюру заказчику - будет страшно интересно :troll: но всё закончится хорошо и если начнём в августе - к НГ уже будут видны результаты в скорости и качестве работы

Затем, 14 августа РП написала, что заказчик готов выслушать наше предложение и мы назначили встречу.

К встрече я составил подробный план реинжиниринга и подготовил презентацию, которая содержала:

. "Погоны" - мой опыт, три успешных кейса реинжиниринга схожего масштаба, работу над Эргономичным подходом;
. Вышеописанные проблемы проекта.
  При том приземлённые на конкретные цифры - сколько заняли конкретные задачи и сколько обычно занимаю аналогичные задачи, к каким конкретным багам привели эти проблемы, в целом статистику по багам в Проекте Э и других моих проектах;
. Предлагаемые альтернативные технические решения;
. Подробный план выполнения реинжиниринга.

Приведя аргументы в пользу того, что реинжиниринг в принципе надо делать, я закинул идею заодно поменять и стек и обосновал это тем, что разница в трудозатратах не такая большая, а в сроках и цене на самом деле будет выигрыш за счёт наличия хороших кадров внутри компании.

И в конце показал план выполнения реинжиниринга, суть которого сводилась к: "Для вас эти работы будут проявляться только в том, что скорость разработки будет постепенно расти, а количество багов - падать".

Заказчик сказал, что очень интересно и надо подумать.
И ушёл.
На месяц с лишним.

А 23 сентября РП и аккаунт на встрече с топ-менеджментом заказчика договорились о старте работ по реинжинирингу.

.Что я вынес для себя
[sidebar]
****
При написании этого поста я прямым текстом спросил у заказчика о том, что повлияло на его положительное решение и вот его ответ:

> В первую очередь сроки реализации доработок для старой архитектуры, а так же ваша презентация, она была довольно убедительной.
  Желание повысить качество и быстродействие системы.

Так же по моему опыту других проектов реинжиниринга отдельных подсистем, могу сказать, что бизнес идёт на это в двух случаях:

. Очевидные операционные проблемы (производительность и стабильность), на которые жалуются клиенты и аргументированное обоснование того, что они не могут быть решены в рамках текущей архитектуры/технологий подсистемы;
. Серьёзные изменения в требованиях, когда даже для заказчика очевидно, что это практически новая фича.

Однако, я думаю что наличие проблем является необходимым, но недостаточным условием для того, чтобы бизнес согласился на реинжиниринг.
А вот достаточным условием является доверие владельца продукта к вам.
Он должен верить вашим словам о невозможности решить проблему локальными изменениями, верить что вы справитесь с задачей, верить что решение действительно исправит проблемы и верить, что вы действуете в его интересах.

На мой взгляд, презентация показалась убедительной (и соответсвенно это стоит взять в следующий раз в аналогичной ситуациии), потому что:

. Я продемонстрировал собственную экспертизу - выраженную как в годах общего стажа, так и конкретных успешных кейсах выполнения реинжиниринга схожего масштаба;
. Я был уверен в успехе и транслировал эту уверенность в метасообщении;
. Я приземлил проблемы кодовой базы на конкретные задачи и цифры;
. У меня было чёткое понимание того, что надо сделать по другому для решения проблем;
. Я подготовил подробный план реинжиниринга;
****

== Планирование реинжиниринга

В первую очередь хочу предупредить: я не профессиональный менеджер и при планировании реинжиниринга импровизировал на ходу.
В моём случае это сработало и - если у вас нет другого варианта - вы можете пойти по тому же пути.
Если же вы сами эксперт в управлении - лучше придерживайтесь своего мнения:)
А если вы не эксперт, но можете делегировать эту работу эксперту - я бы на вашем месте так и сделал.

Импровизацию я начал с того, что попросил одного из разработчиков построить граф зависимостей оригинальной системы:

image::dependency-graph.png[]

По факту это просто перечень REST-эндпоинтов (зелёные прямоугольники), RPC-эндпоинтов (синие) и обработчиков событий (красные) с обозначением вызовов, которые выполняются в процессе их исполнения.
Затем я пробежался по ним беглым взглядом и оценил в "майках" - XS (4 часа), S (8 часов), M (24 часа), L (40 часов), XL (80 часов) - и визуализировал "размерный ряд" насыщенностью цвета прямоугольника.

"Линейка" при этом была следующая:

. XS - Один тривиальный SQL-запрос или RPC-вызов;
. S - Два-три тривиальных SQL-запроса и/или обращения к другому сервису;
. M - Бизнес-логика не влазит на один экран;
. L - Применялся в двух случаях, если:
.. Это был первый эндпоинт сервиса;
.. Я не мог сходу понять структуру и/или детали поведения эндпонита (понимая, при этом его эффекты);
. XL - у меня был только один.
  Это был метод добавления событий, их было семь видов, каждый из которых мапился на таблицу с PostgreSQL-наследованием и имел не совпадающую по структуре входящую DTO-шку.

Всего получилось работ на 354 xs или 177 человеко/дней.
Это соответствует верхней границе первоначальной оценки в 60-180 дней, однако включает в себя несколько новых фич на ~60 человеко/дней, которые ме успели сделать к моменту выполнения детальной оценки.

После этого я нарезал все прямоугольники на спринты.
Задачи в спринты я заталкивал довольно оптимистично, поэтому их получилось восемь штук по 160 человеко/часов в каждом - то есть всего 160 человеко/дней.
Но решил, что пускай мы лучше будем целиться в срок с запасом и первый план оставил таким.

Нарезку я делал интуитивно, руководствуясь следующими принципами (и балансируя между ними):

. Набираем эндпоинты в спринты так, чтобы оценка задач в спринте примерно соответствовала суммарной мощности команды.
  Тут мотивация очевидна, я думаю;
. Идём снаружи внутрь - реинжинирим код только после того, как он перестаёт использоваться в оригинальной системе.
  Это позволило нам, во-первых, не делать RPC-сервер в своей версии (который после перехода на монолит нам не понадобится), а, во-вторых, исключило вероятность того, что мы сломаем старый код не покрытый тестами;
. Фокусируемся на том, чтобы максимально быстро заканчивать каждый микросервис.
  То есть лучше за одну неделю сделать полностью один МС и за вторую полностью второй, чем за неделю сделать два МСа на 50% и за вторую неделю доделать их полностью.
  Это позволило нам минимизировать сложность роутинга в каждый момент времени, быстрее освобождать ресурсы кластера и, главное, минимизировать время, когда с БД одновременно работает старый и новый бэк, что могло привести к неприятным неожиданностям.
. Стараемся все эндпоинты на одном URL сделать за один спринт;
  Для упрощения роутинга и минимизации времени, когда с одними и теми же данными работают оба бэка;
. Эндпоинты на одном URL стараемся делать в таком порядке - GET, DELETE, PUT, POST.
  Это позволио снизить вероятность поломки старого бэка, какой-то "не такой" записью;
. Стараемся, чтобы над одним МСом (хотя бы в рамках спринта) работал только один человек.
  Это позволило нам минимизировать конфликты слияния.

.Что я вынес для себя
[sidebar]
****
. По возможности лучше делегировать планирование профессиональному управленцу;
. Если делегировать невозможно - в аналогичном проекте я бы выполнил планирование также;
. Мёрж конфликты - очень дорогая штука, один из самых кровавых стоил нам двух дней разработки.
  Соответственно надо прикладывать максимум усилий по их исключению.
****

== Процесс работы команды

Про процесс особо рассказать нечего - всё было "как у всех" - псевдоскрам с двухнедельными спринтами и дейликами.

К началу каждого спринта у каждого разработчика был список эндпоинтов, который в идеале он должен был сделать за спринт.
При том, как правило, в спринт я набирал задачи очень оптимистично, и 100% выполнение плана было редкостью.

Сам процесс разработки у нас начинался очень хаотично, проходил разные стадии эволюции, но в итоге устаканился на таком:

. Разработчик создаёт фича-ветку от мастера;
. Разработчик реализует фичу и проходит ревью в ветке;
. Фича-ветка мёржится в мастер;
. Раз в спринт (7-ой из 10 день) объявляется код фриз;
. QA начинает тестировать релиз на dev-стенде, мёржи новых фич в мастер запрещены;
. На 10-ый день (или после того, как QA проверят весь релиз), на мастер вешается тег и тег заливается на stage-стенд, там гоняются автоматические тесты QA-команды;
. Утром 11-ого дня спринта (или на следующий рабочий день, после заливки на стейдж) тег заливается на prod-стенд;
. Код фриз снимается, запускается очередной спринт/релизный цикл.

В конце очередного спринта я подбивал факт.
Первые спринтов 5-6 я действовал по принципу всё или ничего - фича переносится в факт только полностью и после того как пройдёт QA и окажется stage-стенде.

Мотивацией этому было то, что я очень боялся, что мы свалимся в имитацию бурной деятельности без видимых результатов для заказчика.
Однако такой подход привёл к тому, что у нас пару спринтов получилось 0 единиц сделанной работы и в целом видимая мощность команды сильно прыгала из-за чего сложно было понять попадаем ли мы в план.
Поэтому где-то в середине реинжиниринга я перешёл на пропорциональный перенос работ в план - субъективно определял по собственным ощущениям или ощущениям разработчика процент решения задачи и соответсвующий процент "майки" заносил в факт.

Другая техника, которой я боролся за минимизацию времени доведения задач до прода - дейлики.
И да, как и у всех - у нас были дейлики.

Но в отличие от всех _[команд, в котрых я работал]_, на дейликах мы рассказывали не кто чем занимался и будет заниматься, а выясняли кто что должен сделать чтобы каждая задача в процессе максимально быстро оказалась в проде.
И эта техника сработала в том плане, что мы весь реинжиниринг дейлики проводили в таком формате и все остались довольны.

Сложно сказать, чтобы было, если бы мы проводили обычные дейлики, но субъективно мне такой формат ближе (фокусом на достижении целей, а не максимизацией утилизации ресурсов) и для себя я выбрал этот формат дефолтным.

Так же стоит упомянуть о месте в котором мне просто повезло, а вам может не повезти.
Без каких-либо усилий с моей стороны, заказчик практически заморозил разработку бэка на время реинжиниринга и соответсвенно у нас не было цели, убегающий за горизонт.
В этот раз мне повезло, но в следующий раз я возьму этот вопрос на контроль.

.Что я вынес для себя
[sidebar]
****
. Спринты прекрасно работают для реинжиниринга, так как внём нет извечной проблемы, когда по среди спринта от заказчика прилетает горячий пирожок, который надо было сделать вчера;
. Отслеживать прогресс надо процентами выполнения задач, а не штуками;
. Дейлики по задачам мне подходят больше, чем дейлики по людям;
. Важно приложить максимум усилий к заморозке разработки оригинальной кодовой базы, либо при каждой измененеии проговараривать с заказчиком как это повлияет на срок и бюджет реинжиниринга.
****

== Что пошло не плану

=== Спринты

Первое что пошло не по плану - скорость работы команды в первые два спринта.
При оценке задач я ориентировался на среднюю скорость работы и не учёл несколько факторов осложнявших старт:

. В новом проекте приходилось сетапать много "инфраструктурного" кода (например, запуск тестконтейнеров без поломки кэша контектсов) и решать уникальные задачи (например, работу с несколькими DataSource в Spring Data JDBC);
. Команда видела C# в первый и для юниоров читать код на незнакомом языке было довольно сложно;
. Команда в целом слабо ориентировалась в кодовой базе оригинального проекта;
. Приходилось искать решения не типичных проблем из-за наследия .net-бэка (например, обработку CamelCase enum-ов в (де)сериализации DTO).

Из-за этого за первые два спринта (или 25% времени) мы осилили сделать только 5% работы.
Поэтому после второго спринта пришлось сказать, что это была "разминка" и вот теперь оставшиеся 95% работы мы точно сделаем за 8 спринтов.
И в этот раз в целом всё пошло более-менее поплану и в эти 8 спринтов мы уложились, единственное что в последнем спринте одному разработчику пришлось устроить 24 часовой хакатон (по собственной инициативе).

=== Тестирование

==== Разработчиками

Вообще Эргономичный подход предполагает план тестирования.
Он пока что публично не описан, но очень кратко его можно описать следующими принципами:

. Тестируется система в конфигурации максимально приближенной к боевой;
  Мокаются только внешние и дорогие или нестабильные зависимости (например, внешний сервис отправки почты), и то на уровне HTTP.
. Тесты взаимодействуют с системой в обход публичного API только в крайних случаях.
  По умолчанию и сетап и действие и верификация выполняются через публичное API.
. Тесты пишутся исходя из сценариев использования - каждый юзкейс в ТЗ, должен быть покрыть тестом;
. Все задокументированные ошибки API должны быть покрыты тестами (тут, при необходимости, допускается использование моков);
. В бизнес-логике (реализованной в ядре в чистых функциях без ввода-вывода) тестами должны быть покрыты все ветки.
  Если бизнес-логика развесистая, её допустимо тестировать в обход публичного API и напрямую вызывать функции ядра.

И в моей практике эти принципы работают очень хорошо - по статистике в моих проектах команда QA находит мажорные баги примерно раз в три месяца.
Под мажорными я понимаю баги, которые могли бы затронуть большинство пользователей.

Но в Проекте Э пришлось отойти от этих принципов.
И пожалеть об этом.

Честно говоря я уже не помню конкретных причин (дело было почти год назад), но я не стал в тестах поднимать контейнеры старого бэка.
Скорее всего из-за того что [быстро] не придумал как натравить старый бэк на БД, поднимаемой testconainers.

И из-за того, что мы шли снаружи внутрь и начинали с методов чтения, у нас не было ручек для сетапа фикстуры тестов и верификации через публичное API.
Поэтому тестировать я планировал не сценарии использования, а отдельные эндпоинты.

Соответственно, новый план тестирования был такой:

. Сначала пишем тест на отдельный эндпоинт, который проходит на старом бэке, поднятом разработчиком руками;
. Переводим тест на вызов нового бэка;
. Выполняем реинжиниринг этого эндпоинта;

Но практически сразу в этом плане обнаружилась дыра - как сетапить фикстуру?
Через публичное API нельзя, так как его не будет на новом бэке.
А через БД нельзя, так как было не понятно как натравить старый бэк на базу в testcontainers.

В итоге мы писали тесты сразу на отдельные эндпоинты в новом бэке и сетапили фикстуру SQL-скриптами.
А RPC-вызовы к старому бэку мокали.

Кроме того, из соображений минимизации сроков реинжиниринга, мы отказались от покрытия тестами негативных сценариев.

И за оба решения поплатились большим (84 штуки за 5.5 месяцев) количеством багов.
Большинство багов было связано с нарушением обратной совместимости, но были и баги в негативных сценариях и баги вида "тесты на метод А проходят, тесты на метод Б проходят, а вот когда фронт зовёт метод А, а потом метод Б - всё взрывается".

Баги обратной совместимости мы в конечном итоге победили такой схемой:

. Перед старом работ над эндпоинтом, команда QA-пишет тест на структуру ответа в Postman;
. В МР разработчик прикладывает два скриншота - как тест проходит с новым и старым бэком.

Но, незадолго до введения этого правила я уволил стажёра (спойлер 😱), которая генеряла большинство таких багов, поэтому сложно сказать, что внесло больший вклад.

А ошибки в сценариях использования (как негативных, так и позитивных) мы сейчас постепенно изводим возвратом к принципам тестирования ЭП.

Так же мы отдельно поплатились за сетап БД SQL-скриптами.
Во-первых, изначально для моков старого бэка ответы генерировались из чёрт знает каких данных (текущего состояния БД на рабочей машине разработчика).
Соответственно, когда мы эти методы переносили в новый бэк, то для написания скриптов сетапа фикстуры приходилось героически определять входные данные, которые должны быть поданы в операцию чтобы получить заданный результат.

Второй проблемой, актуальной до сих пор стала хрупкость тестов.
В время реинжиниринга она проявлялась в том, что при переносе на новый бэк внутреннего эндпоинта приходилось прописывать скрипты сетапа БД для него во все тесты, в рамках которых этот эндпоинт вызывался.
А сейчас - при изменении схемы БД приходится править сетап фикстуры для множества тестов.
Но это мы потихоньку прибераем, переводя тесты на использование публичного API.

Первую проблему мы частично решили введением "эталонной БД" - взяли дамп с одного из стендов и для генерации мок-данных запускали бэк на нём.

А с хрупкостью тестов живём до сих пор, переводя их на публичное АПИ по мере появления проблем.

==== Командой QA

План тестирования командой QA сводился к паре фраз: "Тестировать будем на дев стенде и стейдже. На деве - через Постаман, на стейдже - через МП".

Но тоже довольно быстро уткнулись в дыру в этом плане - как тестировать эндпоинт?

На момент начала реинжиниринга, бэк-команда видела проект в первый раз, а команды QA и мобильной разработки работали с ним четыре месяца.
В итоге определение сценариев, которые мог затронуть эндпоинт и тест кейсов, которыми его можно проверить превращалось в целую эпопею.

Эту проблему мы так и не решили до конца проекта реинжиниринга.
Буду благодарен, если расскажете в комментариях хороший способ её решения.

=== Выгрузки

Я сильно ошибся в оценке реализации пары фич.
Это две схожие фичи в админке, которые позволяют просматривать списки пациентов и событий дневников.
Казалось бы - что там делать?

Проблема с ними была в том, что данные лежали по разным базам, данных планируется много (до десятков миллионов строк) и при этом надо обеспечить стандартные фичи - пагинацию, сортировку по любому полю и фильтрацию по любому набору полей.
Плюс по требованиям надо было обеспечить выгрузку в xlsx с лимитом на количество строк равным лимиту самого формата - чуть больше одного миллиона.
В итоге нам пришлось руками делать https://en.wikipedia.org/wiki/Block_nested_loop[block nested loop join], о чём я чуть подробнее написал в link:++{{<ref "microposts/23/06/streaming-join">}}++[отдельном микропосте].

В результате вместо запланированных 104 часов на эту работу ушло 175.75 часов.

=== Баги .net-бэка

При планировании я совсем не учитывал поддержку изначальной версии системы.
И хотя разработка была заморожена и новых фич мы не пилили - несколько раз в kotlin-команду прилетали старые баги оригинальной системы, которые проявились только после появления реальных пользователей.
Но нам повезло, багов было не много и они были простые и отъели не так много времени.

=== Стажёр

По среди реинжиниринга мне пришлось уволить стажёра.
Вообще, положа руку на сердце, её надо было уволить намного раньше, но я всё давал шансы.
Пока она не пропала на несколько дней.
И даже тогда я дал ещё один шанс, но как только она тут же снова пропала - моё терпение лопнуло.

Удивительно (на самом деле нет) - но на скорость работы команды это никак не повлияло.
Видимо та польза, которую она приносила, полностью компесировалась проблемами которые, которые она порождала в процессе работы - мучительно долгие ревью, больше количество ошибок, иногда код который проходил только тесты, написанные для подтверждения его работоспосбоности, а не подтверждения его соответствия требованиям.

.Что я вынес для себя
[sidebar]
****
. Первые один-два спринта будут блинами комом и надо быть готовым (заложить в план) к тому, что их цели не будут выполнены даже на 50%;
. Эндпоинты, которые на глаз оцениваются более чем в три дня работы надо всё-таки детально проектировать и декомпозировать до подзадач размером до одного дня;
. Надо придерживаться принципов тестирования Эргономичного подхода - писать тесты на сценарии использования, писать тесты через публичное API, покрывать тестами негативные кейсы;
. Перед началом реинжиниринга надо построить карту, по которой можно быстро определять тесткейсы, которые позволят протестировать каждый эндпоинт;
. Даже если заморозить разработку оригинальной системы, она всё равно может потребовать ресурсов на поддержку;
. Перед стартом проекта надо подумать о своей команде - всем ли я доверяю, все ли дойдут до конца, планируются ли у кого-то отпуск?
  Выявленные риски стоит заложить в план, в виде люфта на решение проблем и заранее продумать план, что делать если они выстрелят;
****

== Факапы в проде

Для начала надо прояснить что я имею ввиду под факапом и продом.

Под факапом я понимаю проблему конечных пользователей, с которой к нам пришёл заказчик.

Касательно прода - это окружение, которым пользуется заказчик и живые пользователи, и у нас это не так страшно, как может показаться.
Первые два наиболее багоёмких месяца работы реальных пользователей у нас не было - приложением пользовались буквально несколько человек со стороны заказчика и близких ему врачей.

Живые пользователи, в количестве ста человек к нам пришли в начале января.
И далее был линейный рост примерно по сто человек в месяц.

=== Приглашение в наблюдатели

Первый факап в проде у нас случился после первого же релиза нового бэка.

У нас есть функциональность приглашения пользователя в наблюдатели по емейлу.
В оригинальном бэке она работала так:

. Сервис share идёт в сервис accounts и смотрит зарегестрирован ли пользователь с таким емейлом;
. Сервис share публикует :[line-through]#событие# команду SendAccountShareInviteLinkIntegrationEvent, куда передаёт флаг accountExists
. Сервис email-notifications формирует ссылку с этим флагом и отправляет письмо на указанный емейл;
. Пользователь проходит по ссылке;
. Фронт смотрит на флаг и либо редиректит пользователя на форму ввода пароля, либо на главную/форму аутентификации.

И при реинжиниринге во входящей DTO-ке разработчик потерял "s" в имени флага.
Занавес.

Проблема ещё усугубилось тем, что в это же время и в этой же функциональности нашли и починили баг на фронте, и мы очень долго разводили кто где ошибся.

=== Поиск наблюдаемого

Второй факап у нас случился уже ближе к концу реинжиниринга.

У врача есть возможность искать своих пациентов.
В старом бэке поиск выполнялся и по имени и по логину.
А при реинжиниринге потеряли поиск по имени.
Занавес.

=== Обработка протухших токенов

Последний релиз реинжиниринга у нас тоже отметился факапом.

МП в плане обновления токена, МП у нас реактивные - рефрешат его по 401-ой ошибке, а не заранее.
А при реализации рефреша токена разработчик пропустил, что библиотека парсина JWT будет выбрасывать исключение и в случае валидного, но протухшего токена.
А все неожиданные исключения у нас мапятся на 500.
Занавес.

---

Примечательно, что всех трёх факапов можно было бы избежать, если бы мы придерживались принципам тестирования ЭП.

Факап с приглашением бы отловили когда поняли, что тесты двух юз кейсов должны отличаться флагом в ссылке в письме, добавили бы забытую проверку и обнаружили, что один из них не проходит.

Факап с поиском очевидным образом бы отловил тест кейса поиска по имени.

Факап с протухшими токенами бы отловил тест кейса обработки протухшего токена.

.Что я вынес для себя
[sidebar]
****
. И снова - надо придерживаться принципов тестирования Эргономичного подхода.
****

== Результаты

Итого, проект реинжиниринга длился ~5.5 месяцев с 1 ноября 2022 года по 17 апреля 2023 года.
Общие трудозатраты на разработку, поддержку и коммуникации составили 1402.75 часа (175 человеко/деней).

[NOTE]
====
Точность попадания в подробную оценку может показаться феноменальной - 175 против 177 дней.
Однако на самом деле оценка была несколько завышена относительно идеальной - за 175 дней помимо того, что было запланировано, мы выполнили ещё и реинжиниринг сервисов точек продаж и устройств, которые я не учитывал в подробной оценке.
====

В результате у нас получилось:

. 23,944 строк кода;
. 730 классов;
. 234 теста (преимущественно интеграционных);
. 100% покрытие эндпоинтов тестами;
. 93.2% покрытия строк кода тестами;
. 1:30 минут полное время сборки, включая все тесты кода, тесты архитектуры, detekt, сборку и верификацию покрытия кода;
. 81 баг, который нашли мы или QA;
. 3 бага, которые нашли пользователи или заказчик.

Стоило ли оно того?
Безусловно да.

Через три месяца после завершения реинжиниринга я проанализировал задачи в Jira и написал об этом link:++{{<ref "microposts/23/07/project-e-retro-v2">}}++[подробный пост].
Главный вывод этого поста: после завершения реинжиниринга мы стали работать в два раза быстрее, в том числе за счёт того, что стали допускать в два раза меньше ошибок.

.Что я вынес для себя
[sidebar]
****
. Проекты до человеко-года на МСА будут дороже минимум на 30%, аналогичных проектов на монолите;
. Автоматизация тестирования как минимум в двое сокращает количество ошибок;
. При наличии сильного лида, замотивированность разработчиков важнее квалификации.
****

== Заключение

Какие выводы можно сделать из этой истории?

. Главный вывод для меня - по Эргономичному подходу можно успешно делать проекты в 1+ человеко/год;
. Если вы хотите, чтобы ваши последователи без проблем убедили заказчика в том что вы не справились со своей задачей, то:
.. начните небольшой проект с микросервисной архитектуры и сделайте её такой, чтобы каждая операция делала по 2-3 синхронных вызова в другие сервисы;
.. не напишите ни одного теста
.. примените вертикальную архитектуру;
. Если же вы хотите, чтобы ваши последователи были вам благодарны, то:
.. Прислушайтесь к https://www.martinfowler.com/bliki/MonolithFirst.html[Фаулеру], https://www.oreilly.com/library/view/building-microservices/9781491950340/[Ньюману] и https://microservices.io/post/microservices/patterns/2020/10/18/microservices-are-a-mistake.html[Ричардсону] и начните новый проект с монолита.
   А чтобы упростить переход на МСА (или вообще избежать его) - прислушайтесь к https://www.youtube.com/watch?v=5OjqD-ow8GE[Брауну] и link:++{{<ref "posts/22/08/ergonomic-decomposition#_пакетирование_по_объектам_ака_объектно_ориентированная_декомпозиция">}}++[мне] и сделайте его модульным.
.. Покройте 100% своих эндпоинтов тестами.
   А чтобы они были показательными и стабильными - прислушайтесь к https://martinfowler.com/articles/mocksArentStubs.html[Фаулеру], https://blog.cleancoder.com/uncle-bob/2014/05/10/WhenToMock.html[Мартину], https://youtu.be/z9quxZsLcfo?t=1270[Беку], https://www.ozon.ru/product/printsipy-yunit-testirovaniya-horikov-vladimir-553281795/?sh=VykwEHH-fw[Хорикову] и link:++{{<ref "posts/21/03/210321-project-l-testing">}}++[мне] и пишите их в стиле https://habr.com/ru/companies/jugru/articles/571126/[классической школы]
.. Возьмите простую архитектуру - хотя бы слоёную (внутри модулей).
   А чтобы упростить тестирование и понимание кода - лучше возьмите функциональную/неизменяемую архитектуру (описание https://habr.com/ru/articles/571668/[без монад] и https://habr.com/ru/companies/jugru/articles/341460/[с ними], link:++{{<ref "posts/21/10/211018-ergo-approach-post#_%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D0%B0_%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B">}}++[моя версия]).
. Реализовать свою мечту и переписать чужой легась - можно;
. Попасть в бюджет при этом - тоже можно;
. Чтобы убедить владельца продукта в необходимости реинжиниринга, надо:
.. Заслужить его доверие;
.. Оперировать понятными для него фактами;
.. Приземлять проблемы легаси системы на деньги;
.. Показать, что вы понимаете в чём конкретно заключены проблемы легаси системы, что надо сделать по другому, чтобы их решить.
. Простая методика оценки работ посредством умножения на константу количества таблиц и эндпоинтов хорошо работает для типовых crud-приложений;
. При планировании надо учитывать, что скорость команды на старте будет очень низкой, но будет постоянно расти;
. Первый год разработки на микросервисах дороже разработки на монолите.
  Минимум на 30%;
. Автоматизация тестирования снижает количество багов и трудозатрат на их устранение.
  Минимум в два раза;
. Мотивация команды имеет огромное влияние на трудозатарты.
  От 30% дополнительных трудозатрат в случае низкой мотивации.

В следующем посте я расскажу как мы всё это делали фактически:

. как структурировали приложение;
. как боролись за низкую сцепленность;
. как тестировали;
. как эволюционировала наша модель ветвления;
. как деплоились;
. какие были технические сложности и как мы их преодолевали;
. какие факапы у нас были в проде.
